# BAYESIAN ENTROPY FOR LLM HALLUCINATION DETECTION

# ğŸ§  LLM-HALLUCINATION

This repository explores **hallucination detection and mitigation** in Large Language Models (LLMs) using Bayesian estimators, adaptive sampling, and evaluation over multiple QA datasets (SQuAD, SVAMP, TriviaQA).

---

## ğŸ“ Project Structure
```
LLM-HALLUCINATION/
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json
|
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generations/
â”‚   â”œâ”€â”€ meanings/
|   |   â”œâ”€â”€ cosine_sim
|   |   â”œâ”€â”€ pearson_sim
|   |   â”œâ”€â”€ rbf_sim
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ squad_prompts.json
â”‚   â”‚   â”œâ”€â”€ svamp_prompts.json
â”‚   â”‚   â””â”€â”€ triviaqa_prompts.json
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ SQuAD/
â”‚       â”‚   â”œâ”€â”€ dev-v2.0.json
â”‚       â”‚   â””â”€â”€ SQuAD.ipynb
â”‚       â”œâ”€â”€ SVAMP/
â”‚       â”‚   â”œâ”€â”€ SVAMP.json
â”‚       â”‚   â””â”€â”€ SVAMP.ipynb
â”‚       â””â”€â”€ TriviaQA/
â”‚           â”œâ”€â”€ TriviaQA.json
â”‚           â””â”€â”€ TriviaQA.ipynb
â”œâ”€â”€ pdfs/
â”œâ”€â”€ src_code/
â”‚   â”œâ”€â”€ bayesian_estimator.py
|   â”œâ”€â”€ eval_tune.py
|   â”œâ”€â”€ load_model_output.py
|   â”œâ”€â”€ meaning_mapper.py
|   â”œâ”€â”€ original_clusters.py 
|   â”œâ”€â”€ run_adaptive.py 
|      
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

```




---

## âš™ï¸ Setup

### 1. Create Conda Environment
```bash
conda create -n llmhall python=3.11
conda activate llmhall
pip install -r requirements.txt
```

If youâ€™re using llama-cpp-python or similar:  Install Visual Studio Build Tools with Desktop Development with C++.
```pip install llama-cpp-python --force-reinstall --no-cache-dir```

---



## THEORY
---
 ## step 1

For each prompt, generate multiple responses from the language model

Group the responses into semantic clusters (each cluster represents one meaning)

Let the number of observed meanings be k_obs


 ## step 2

For each prompt, construct a probability distribution over the possible total number of meanings K, conditioned on k_obs

Enforce the constraint K â‰¥ k_min, where k_min is the minimum number of meanings observed for the prompt, k_min = k_obs


 ## step 3

For each possible value of K, construct a Dirichlet distribution over the K meaning probabilities

Enforce lower bounds on meaning probabilities using the summed likelihoods of observed sequences belonging to each meaning


 ## step 4

For each Dirichlet distribution, compute Shannon entropy

This induces a probability distribution over entropy values

Integrate hierarchically over K to compute the expected semantic entropy and the variance of semantic entropy



 ## step 5

Use the estimated semantic entropy as a signal to determine whether a response is likely reliable or hallucinated

---






